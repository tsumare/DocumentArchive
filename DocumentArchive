#!/usr/bin/env python3
import argparse
import base64
import binascii
import boto3, botocore
import calendar
import collections
import datetime
import dateutil.parser
import hashlib
import itertools
import json
import marshal
import os
import pprint
import re
import subprocess
import sys
import time
import urllib.parse
import uuid

try:
	CONFIG = json.load(open(os.path.join(os.environ.get('HOME','.'), '.documentarchive.conf'), 'r'))
except Exception as e:
	print('Can\'t load config file {0}: {1}'.format(os.path.join(os.environ.get('HOME','.'), '.documentarchive.conf'), e))
	print('Config format:')
	print(json.dumps({
		'bucket': 'bucket.for.archive',
		'pastebucket': 'optional bucket for generating short urls, else: delete or set to null',
		}, indent='\t'))
	print()
	sys.exit(1)

s3session = boto3.Session()
s3client = s3session.client('s3')
s3 = s3session.resource('s3')
sdb = s3session.client('sdb')

def cmd_rebuild(args):
	sdb.delete_domain(DomainName=CONFIG['simpledb_domain'])
	sdb.create_domain(DomainName=CONFIG['simpledb_domain'])
	for item in itertools.chain.from_iterable(map(lambda x: x['Contents'], s3client.get_paginator('list_objects_v2').paginate(Bucket=CONFIG['bucket']))):
		obj = s3.Object(CONFIG['bucket'], item['Key'])
		sdb_attrs = dict(obj.metadata)
		if 'uuid' not in sdb_attrs:
			continue
		print('{}  {}'.format(sdb_attrs['uuid'], item['Key']))
		sdb_attrs['S3_ContentType'] = obj.content_type
		sdb_attrs['S3_Key'] = item['Key']
		sdb_attrs['S3_Sequencer'] = '0 {}'.format(item['Key'])
		Attributes = []
		for k,v in sdb_attrs.items():
			if k == 'tags':
				for tag in map(lambda x: x.strip(), sdb_attrs['tags'].split(',')):
					Attributes.append({'Name': 'tags', 'Value': tag, 'Replace':True})
			else:
				Attributes.append({'Name': k, 'Value': v, 'Replace':True})
		sdb.put_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=sdb_attrs['uuid'], Attributes=Attributes)
		sdb.delete_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=sdb_attrs['uuid'], Attributes=[{'Name': 'tags', 'Value': ''}])
	s3.Bucket(CONFIG['bucket']).Object('cache.json').delete()

def parse_attributes(Attributes):
	if isinstance(Attributes, dict):
		Attributes = Attributes.get('Attributes',[])
	def parse_date(d):
		if not d:
			return None
		return datetime.datetime.strptime(d,'%Y-%m-%d').date()
	item = {}
	for k,v in map(lambda x: (x['Name'], x['Value']), Attributes):
		if k.endswith('-date'):
			v = None if not v else datetime.datetime.strptime(v, '%Y-%m-%d').date()
		if k not in item:
			item[k] = v
		else:
			if not isinstance(item[k], list):
				item[k] = [item[k]]
			item[k].append(v)
	if item:
		item.setdefault('tags',[])
	return item

def sdb_select(SelectExpression, count_query=False):
	if count_query:
		return sum(map(lambda x: parse_attributes(x)['count'], sdb_select(SelectExpression, count_query=False)))
	kwargs = { 'SelectExpression': SelectExpression }
	while True:
		ret = sdb.select(**kwargs)
		yield from ret.get('Items',[])
		if 'NextToken' in ret:
			kwargs['NextToken'] = ret['NextToken']
		else:
			break

def cmd_open(args):
	item = parse_attributes(sdb.get_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=args.uuid))
	if not item:
		print('UUID not known')
		return
	url = s3client.generate_presigned_url('get_object', Params={
		'Bucket': CONFIG['bucket'],
		'Key': item['S3_Key'],
		'ResponseContentDisposition': 'filename="{0}"'.format(item['original-filename']),
		}, ExpiresIn=300)
	subprocess.call(['xdg-open', url])

def cmd_save(args):
	item = parse_attributes(sdb.get_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=args.uuid))
	if not item:
		print('UUID not known')
		return
	if args.output.endswith('/'):
		args.output += '{uuid} {original-filename}'
	filename = args.output.format(extension=os.path.splitext(item['original-filename'])[1], **item)
	os.makedirs(os.path.abspath(os.path.dirname(filename)), exist_ok=True)
	s3.Bucket(CONFIG['bucket']).Object(item['S3_Key']).download_file(filename)
	print(filename)

def cmd_backup(args):
	try:
		os.mkdir(args.dest)
	except FileExistsError:
		pass
	oldumask = os.umask(0o077)
	for item in itertools.chain.from_iterable(map(lambda x: x['Contents'], s3client.get_paginator('list_objects_v2').paginate(Bucket=CONFIG['bucket']))):
		keypath, keybase = os.path.split(item['Key'])
		keybase, keyext = os.path.splitext(keybase)
		keymtime = item['LastModified'].strftime('%Y%m%dT%H%M%S')
		fnfmt = os.path.join(args.dest, '{path}/{base}-content-{{sha}}{ext}'.format(path=keypath, base=keybase, mtime=keymtime, ext=keyext))
		mdfn = os.path.join(args.dest, '{path}/{base}-metadata-{mtime}.json'.format(path=keypath, base=keybase, mtime=keymtime, ext=keyext))
		print(mdfn)
		if os.path.exists(mdfn):
			try:
				metadata = json.load(open(mdfn,'r'))
				if os.path.exists(fnfmt.format(sha=metadata['sha256'])):
					continue
			except:
				pass
		obj = s3.Object(CONFIG['bucket'], item['Key'])
		metadata = dict(obj.metadata)
		if 'uuid' not in metadata:
			continue
		metadata['S3_ContentType'] = obj.content_type
		metadata['S3_Key'] = item['Key']
		if os.path.exists(mdfn):
			continue
		print('{}  {}'.format(metadata['uuid'], item['Key']))
		if os.path.exists(mdfn):
			continue
		pprint.pprint(metadata)
		os.makedirs(os.path.join(args.dest, keypath), exist_ok=True)
		json.dump(metadata, open(mdfn+'~','w'), sort_keys=True, indent='\t')
		if not os.path.exists(fnfmt.format(sha=metadata['sha256'])) or metadata['sha256'] != hashlib.sha256(open(fnfmt.format(sha=metadata['sha256']),'rb').read()).hexdigest():
			obj.download_file(fnfmt.format(sha=metadata['sha256'])+'~')
			os.rename(fnfmt.format(sha=metadata['sha256'])+'~', fnfmt.format(sha=metadata['sha256']))
		os.rename(mdfn+'~', mdfn)
	os.umask(oldumask)

def cmd_delete(args):
	item = parse_attributes(sdb.get_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=args.uuid))
	if not item:
		print('UUID not known')
		return
	s3.Bucket(CONFIG['bucket']).Object(item['S3_Key']).delete()
	print(args.uuid)

def cmd_geturl(args):
	item = parse_attributes(sdb.get_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=args.uuid))
	if not item:
		print('UUID not known')
		return
	url = s3client.generate_presigned_url('get_object', Params={
		'Bucket': CONFIG['bucket'],
		'Key': item['S3_Key'],
		'ResponseContentDisposition': 'filename="{0}"'.format(item['original-filename']),
		}, ExpiresIn=int(args.expires))
	if CONFIG.get('pastebucket', None) is None:
		print(url)
	else:
		shapart = binascii.a2b_hex(item['sha256'][:16])
		tag = binascii.b2a_base64(shapart).decode('ascii').replace('\n','').replace('+','-').replace('/','_').replace('=','')
		tag += '.{0}'.format(int(time.time())+args.expires)
		if args.long:
			tag += '/' + item['original-filename']
		else:
			tag += os.path.splitext(item['original-filename'])[1]
		s3.Bucket(CONFIG['pastebucket']).Object(tag).put(Body=b'', ContentType='text/plain', WebsiteRedirectLocation=url)
		print('https://{pastebucket}/{tag}'.format(pastebucket=CONFIG['pastebucket'], tag=tag))

def cmd_info(args):
	item = parse_attributes(sdb.get_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=args.uuid))
	if not item:
		print('UUID not known')
		return
	maxklen = max(map(lambda x: len(x), item.keys()), default=1)
	fmt = '{k:%MKL%s}  {v}'.replace('%MKL%', str(maxklen+1))
	section = 1
	for k, v in sorted(item.items(), key=lambda x: (not x[0].startswith('S3_'), x)):
		if not k.startswith('S3_'):
			if section == 1:
				section = 2
				print()
		if isinstance(v, (set,list)):
			v = ', '.join(sorted(v))
		if isinstance(v, datetime.date):
			v = v.strftime('%Y-%m-%d')
		if isinstance(v, datetime.datetime):
			v = v.strftime('%Y-%m-%d %H:%M:%S %z')
		print(fmt.format(k=k+':', v=v))

def cmd_list(args):
	def queryquote(v):
		return str(v).replace('\\','\\\\').replace('"','\\"')
	query = ['S3_Key is not null']
	if args.category is not None:
		query.append('(category = "{}" or category like "{}/%")'.format(queryquote(args.category), queryquote(args.category).replace('%','\\%')))
	if args.relevant_date is not None:
		query.append('''
			(
				`relevant-date` between "{start_date}" and "{end_date}"
				or
				(
					`relevant-date` is null
					and
					`archive-date` between "{start_date}" and "{end_date}"
				)
			)'''.format(start_date=queryquote(args.relevant_date[0]), end_date=queryquote(args.relevant_date[1])))
	if args.archive_date is not None:
		query.append('`archive-date` between "{}" and "{}"'.format(queryquote(args.archive_date[0]), queryquote(args.archive_date[1])))
	if args.tag:
		query.append(' intersection '.join(map(lambda x: 'tags = "{}"'.format(queryquote(x)), set(args.tag))))
	if args.title is not None:
		if '%' in args.title:
			query.append('title like "{}"'.format(queryquote(args.title).replace('%%','\\%')))
		else:
			query.append('title like "%{}%"'.format(queryquote(args.title)))
	query = 'select * from {} where {}'.format(CONFIG['simpledb_domain'], ' and '.join(query))
	try:
		query = map(parse_attributes, sdb_select(SelectExpression=query))
	except Exception:
		print('Query: {}'.format(query))
		raise
	def resultsortkey(item):
		itmrdate = item.get('relevant-date', item['archive-date'])
		if itmrdate is None:
			itmrdate = item['archive-date']
		return (itmrdate, item['archive-date'], item['title'])
	results = sorted(query, key=resultsortkey)
	result_cat_max_len = max(map(lambda x: len(x['category']), results), default=0)
	for result in results:
		fmtstr = '{item[uuid]} | {reldate} | {archdate} | {item[category]:CAT_MAX_LENs} | {fmt:4s} | {item[title]}'
		fmtstr = fmtstr.replace('CAT_MAX_LEN', str(result_cat_max_len))
		print(fmtstr.format(
			item=result,
			fmt=os.path.splitext(result['original-filename'])[1].lstrip('.').lower(),
			reldate=result['relevant-date'].strftime('%Y-%m-%d') if result.get('relevant-date',None) is not None else '          ',
			archdate=result['archive-date'].strftime('%Y-%m-%d')
			))

def cmd_checklist(args):
	def queryquote(v):
		return str(v).replace('\\','\\\\').replace('"','\\"')
	query = ['S3_Key is not null']
	if args.category is not None:
		query.append('(category = "{}" or category like "{}/%")'.format(queryquote(args.category), queryquote(args.category).replace('%','\\%')))
	if args.relevant_date is not None:
		query.append('''
			(
				`relevant-date` between "{start_date}" and "{end_date}"
				or
				(
					`relevant-date` is null
					and
					`archive-date` between "{start_date}" and "{end_date}"
				)
			)'''.format(start_date=queryquote(args.relevant_date[0]), end_date=queryquote(args.relevant_date[1])))
	if args.archive_date is not None:
		query.append('`archive-date` between "{}" and "{}"'.format(queryquote(args.archive_date[0]), queryquote(args.archive_date[1])))
	if args.tag:
		query.append(' intersection '.join(map(lambda x: 'tags = "{}"'.format(queryquote(x)), set(args.tag))))
	if args.title is not None:
		if '%' in args.title:
			query.append('title like "{}"'.format(queryquote(args.title).replace('%%','\\%')))
		else:
			query.append('title like "%{}%"'.format(queryquote(args.title)))
	query = 'select * from {} where {}'.format(CONFIG['simpledb_domain'], ' and '.join(query))
	try:
		query = map(parse_attributes, sdb_select(SelectExpression=query))
	except Exception:
		print('Query: {}'.format(query))
		raise
	countmap = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))
	periods = set()
	period_max = {}
	for item in query:
		reldate = item.get('relevant-date', item['archive-date'])
		period = reldate.replace(day=1)
		periods.add(period)
		countmap[item['category']][period] += 1
		period_max[item['category']] = max(period_max.get(item['category'], reldate), reldate)

	if not countmap:
		print('No results.')
		return

	reflection = list
	if args.reverse:
		reflection = reversed

	fmt  = '{{:{:d}s}}  '.format(max(map(lambda x: len(x), countmap.keys())))
	fmt += '{:7s}  '*len(periods)
	if args.last:
		fmt += '{:10s}  '
	fmt.strip()
	fmtargs = ['']
	fmtargs.extend(map(lambda x: x.strftime('%Y-%m'), reflection(sorted(periods))))
	if args.last:
		fmtargs.append('last entry')
	print(fmt.format(*fmtargs))
	columnized = {}
	for category, period_counts in countmap.items():
		columns = columnized.setdefault(category, [])
		for period in sorted(periods):
			count = countmap[category][period]
			columns.append({'value': count, 'delta': (count - columns[-1]['value'] if columns else None)})

	for category, columns in sorted(columnized.items(), key=lambda x: list(reversed(list(map(lambda x: x['delta'] or 0, x[1]))))):
		fmtargs = [category]
		fmtargs.extend(map(lambda x: '{value:2d} ({delta:+2d})'.format(**x) if x['delta'] is not None else ('     {value:2d}' if not args.reverse else '{value:2d}      ').format(**x), reflection(columns)))
		if args.last:
			fmtargs.append(period_max[category].strftime('%Y-%m-%d'))
		print(fmt.format(*fmtargs))

def cmd_categories(args):
	categories = filter(lambda x: x is not None, map(lambda x: parse_attributes(x).get('category',None), sdb_select(SelectExpression='select category from {}'.format(CONFIG['simpledb_domain']))))
	categories = sorted(set(categories))
	for category in categories:
		print(category)

def detect_mimetype(fn):
	#try:
	#	import magic
	#	return magic.from_file(fn, mime=True)
	#except ImportError:
	#	print('Please install python3-magic!')
	try:
		import mimetypes
		return mimetypes.guess_type(fn)[0] or 'application/octet-stream'
	except ImportError:
		pass
	return 'application/octet-stream'

def cmd_store(args):
	if not os.path.exists(args.filename):
		print('File not found.')
		sys.exit(1)

	if args.new_category is None and args.category is None:
		print('A category is required!')
		sys.exit(1)
	if args.new_category is None:
		if not sdb_select(SelectExpression='select count(*) from {} where category = "{}"'.format(CONFIG['simpledb_domain'], args.category), count_query=True):
			print('Category does not yet exist.  Misspelled?  Consider --new-category/-C')
			sys.exit(1)
	else:
		args.category = args.new_category

	# Attempt to reserve an ID.  Because we do not set S3_Sequencer, this should
	# not interfere with the metadata update lambda.
	item_uuid = None
	while item_uuid is None:
		item_uuid = str(uuid.uuid4())
		try:
			sdb.put_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=item_uuid, Attributes=[{'Name':'uuid','Value':item_uuid}], Expected={'Name':'uuid','Exists':False})
		except Exception as e:
			print('UUID Reservation Exception (Retrying): {}'.format(e))
			item_uuid = None

	meta = {
			'original-filename': os.path.basename(args.filename),
			'title': args.title or os.path.splitext(os.path.basename(args.filename))[0],
			'category': args.category or '',
			'relevant-date': '' if args.relevant_date is None else args.relevant_date.strftime('%Y-%m-%d'),
			'archive-date': args.archive_date.strftime('%Y-%m-%d'),
			'tags': ','.join(sorted(args.tag)),
			'uuid': item_uuid,
			'sha256': hashlib.sha256(open(args.filename,'rb').read()).hexdigest(),
			}
	content_type = args.content_type or detect_mimetype(args.filename)
	objname = '{relevant_year}/{category}/{relevant_date} {title}/{uuid}{ext}'.format(
			relevant_year=args.archive_date.strftime('%Y') if args.relevant_date is None else args.relevant_date.strftime('%Y'),
			category=meta['category'],
			relevant_date=(meta['relevant-date'] or meta['archive-date']),
			uuid=meta['uuid'],
			sha256=meta['sha256'],
			title=meta['title'].replace('/','_'),
			ext='.'+args.extension if args.extension is not None else os.path.splitext(args.filename)[1],
			)

	pprint.pprint({'file':objname,'meta':meta,'content-type':content_type})
	if not args.dryrun:
		obj = s3.Bucket(CONFIG['bucket']).Object(objname)
		obj.upload_file(
				args.filename,
				ExtraArgs = {
					'Metadata': meta, #dict(map(lambda x: ('x-amz-meta-'+x[0],x[1]), meta)),
					'ContentType': content_type,
					'ServerSideEncryption': 'AES256',
					}
				)

def cmd_tweak(args):
	item = parse_attributes(sdb.get_attributes(DomainName=CONFIG['simpledb_domain'], ItemName=args.uuid))
	if not item:
		print('UUID not known')
		return

	if args.new_category is None and args.category is not None:
		if not sdb_select(SelectExpression='select count(*) from {} where category = "{}"'.format(CONFIG['simpledb_domain'], args.category), count_query=True):
			print('Category does not yet exist.  Misspelled?  Consider --new-category/-C')
			sys.exit(1)
	else:
		args.category = args.new_category

	item_orig_key = item['S3_Key']

	if args.title is not None:
		item['title'] = args.title
	if args.category is not None:
		item['category'] = args.category
	if args.relevant_date is not None:
		item['relevant-date'] = args.relevant_date
	if args.archive_date is not None:
		item['archive-date'] = args.archive_date
	if args.tag:
		item['tags'] = args.tag
	if args.content_type is not None:
		item['S3_ContentType'] = args.content_type

	objname = '{relevant_year}/{category}/{relevant_date} {title}/{uuid}{ext}'.format(
			relevant_year=item['archive-date'].strftime('%Y') if item['relevant-date'] is None else item['relevant-date'].strftime('%Y'),
			category=item['category'],
			relevant_date=(item['relevant-date'] or item['archive-date']).strftime('%Y-%m-%d'),
			uuid=item['uuid'],
			sha256=item['sha256'],
			title=item['title'].replace('/','_'),
			ext=os.path.splitext(item['S3_Key'])[1],
			)

	meta = dict(map(lambda x: (x[0], x[1].strftime('%Y-%m-%d') if isinstance(x[1], (datetime.date, datetime.datetime)) else x[1]), item.items()))
	meta['tags'] = ','.join(sorted(meta['tags']))
	meta = dict(filter(lambda x: not x[0].startswith('S3_'), meta.items()))
	if meta['relevant-date'] is None:
		meta['relevant-date'] = ''

	pprint.pprint({'file':objname,'meta':meta,'content-type':item['S3_ContentType']})
	if not args.dryrun:
		oldobj = s3.Bucket(CONFIG['bucket']).Object(item_orig_key)
		newobj = s3.Bucket(CONFIG['bucket']).Object(objname)
		newobj.copy_from(
				CopySource           = {'Bucket': oldobj.bucket_name, 'Key': oldobj.key},
				Metadata             = meta,
				MetadataDirective    = 'REPLACE',
				TaggingDirective     = 'COPY',
				ContentType          = item['S3_ContentType'],
				#StorageClass        = item['_StorageClass'],
				ServerSideEncryption = 'AES256'
				)
		if (oldobj.bucket_name, oldobj.key) != (newobj.bucket_name, newobj.key):
			oldobj.delete()

def parse_args():
	parser = argparse.ArgumentParser()
	subparsers = parser.add_subparsers(dest='command')
	subparsers.required = True

	def parse_date(date):
		if date.lower() == 'today':
			return datetime.date.today()
		if re.match('^[0-9]{4}-[0-9]{2}-(Z|END)$', date, re.IGNORECASE) is not None:
			date = dateutil.parser.parse('-'.join(date.split('-')[:2]) + '-01')
			return date.replace(day=calendar.monthrange(date.year, date.month)[1])
		return dateutil.parser.parse(date).date()

	def parse_date_range(date):
		date = date.split('~')
		if len(date) == 1:
			date = parse_date(date[0])
			return (date, date)
		elif len(date) == 2:
			return (parse_date(date[0]), parse_date(date[1]))
		else:
			raise ValueError('Date range invalid')

	def parse_duration(duration):
		assert re.match('^([0-9]+|([0-9]+y)?([0-9]+M)?([0-9]+d)?([0-9]+h)?([0-9]+m)?([0-9]+s)?)$', duration) is not None, 'Invalid duration '+duration
		parts = re.findall('[0-9]+[yMwdhms]', duration)
		if not parts:
			return int(duration)
		else:
			duration = 0
			for part in parts:
				duration += int(part[:-1]) * {'y':86400*365, 'M':86400*31, 'd':86400, 'h':3600, 'm': 60, 's':1}[part[-1]]
			return duration

	def parse_tag(tag):
		if ',' in tag:
			raise ValueError('Tags cannot contain \',\'')
		return tag

	sp = subparsers.add_parser('store')
	sp.add_argument('-C','--new-category', action='store', default=None)
	sp.add_argument('-c','--category', action='store', default='Misc')
	sp.add_argument('-d','--relevant-date', action='store', type=parse_date, default=None)
	sp.add_argument('-t','--tag', action='append', type=parse_tag, default=[])
	sp.add_argument('-n','--title', action='store', default=None)
	sp.add_argument('--dryrun', action='store_true')
	sp.add_argument('--extension', action='store', default=None)
	sp.add_argument('--content-type', action='store', default=None)
	sp.add_argument('--archive-date', action='store', type=parse_date, default=datetime.date.today())
	sp.add_argument('filename')
	sp.set_defaults(command_function=cmd_store)

	sp = subparsers.add_parser('tweak')
	sp.add_argument('-C','--new-category', action='store', default=None)
	sp.add_argument('-c','--category', action='store', default=None)
	sp.add_argument('-d','--relevant-date', action='store', type=parse_date, default=None)
	sp.add_argument('-t','--tag', action='append', type=parse_tag, default=[])
	sp.add_argument('-n','--title', action='store', default=None)
	sp.add_argument('--dryrun', action='store_true')
	sp.add_argument('--content-type', action='store', default=None)
	sp.add_argument('--archive-date', action='store', type=parse_date, default=None)
	sp.add_argument('uuid')
	sp.set_defaults(command_function=cmd_tweak)

	sp = subparsers.add_parser('list')
	sp.add_argument('-c','--category', action='store', default=None)
	sp.add_argument('-d','--relevant-date', action='store', type=parse_date_range, default=None)
	sp.add_argument('-t','--tag', action='append', type=parse_tag, default=[])
	sp.add_argument('-n','--title', action='store', default=None)
	sp.add_argument('--extension', action='store', default=None)
	sp.add_argument('--archive-date', action='store', type=parse_date_range, default=None)
	sp.set_defaults(command_function=cmd_list)

	sp = subparsers.add_parser('checklist')
	sp.add_argument('-c','--category', action='store', default=None)
	sp.add_argument('-d','--relevant-date', action='store', type=parse_date_range, default=None)
	sp.add_argument('-t','--tag', action='append', type=parse_tag, default=[])
	sp.add_argument('-n','--title', action='store', default=None)
	sp.add_argument('-r','--reverse', action='store_true')
	sp.add_argument('-l','--last', action='store_true')
	sp.add_argument('--extension', action='store', default=None)
	sp.add_argument('--archive-date', action='store', type=parse_date_range, default=None)
	sp.set_defaults(command_function=cmd_checklist)

	sp = subparsers.add_parser('categories')
	#sp.add_argument('-c','--category', action='store', default=None)
	sp.set_defaults(command_function=cmd_categories)

	sp = subparsers.add_parser('geturl')
	sp.add_argument('-e','--expires', action='store', type=parse_duration, default=3600)
	if CONFIG.get('pastebucket', None) is not None:
		sp.add_argument('-l','--long', action='store_true')
	sp.add_argument('uuid')
	sp.set_defaults(command_function=cmd_geturl)

	sp = subparsers.add_parser('open')
	sp.add_argument('uuid')
	sp.set_defaults(command_function=cmd_open)

	sp = subparsers.add_parser('save')
	sp.add_argument('uuid')
	sp.add_argument('-o','--output', action='store', default='{uuid} {original-filename}')
	sp.set_defaults(command_function=cmd_save)

	sp = subparsers.add_parser('backup')
	sp.add_argument('dest')
	sp.set_defaults(command_function=cmd_backup)

	sp = subparsers.add_parser('dElEtE')
	sp.add_argument('uuid')
	sp.set_defaults(command_function=cmd_delete)

	sp = subparsers.add_parser('info')
	sp.add_argument('uuid')
	sp.set_defaults(command_function=cmd_info)

	sp = subparsers.add_parser('rebuild')
	sp.set_defaults(command_function=cmd_rebuild)
	return parser.parse_args()

args = parse_args()
args.command_function(args)
